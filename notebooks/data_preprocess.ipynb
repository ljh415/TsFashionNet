{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/paper/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/jaeho/SSD/datasets/deepfashion/category_and_attribute_prediction/category_and_attribute_prediction_benchmark/Anno_coarse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list_attr_cloth.txt',\n",
       " 'list_attr_img.txt',\n",
       " 'list_bbox.txt',\n",
       " 'list_category_cloth.txt',\n",
       " 'list_category_img.txt',\n",
       " 'list_landmarks.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img_dir = '/media/jaeho/SSD/datasets/deepfashion/img-001/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_cloth_path = os.path.join(data_dir, 'list_attr_cloth.txt')\n",
    "attr_img_path = os.path.join(data_dir, 'list_attr_img.txt')\n",
    "bbox_path = os.path.join(data_dir, 'list_bbox.txt')\n",
    "category_cloth_path = os.path.join(data_dir, 'list_category_cloth.txt')\n",
    "category_img_path = os.path.join(data_dir, 'list_category_img.txt')\n",
    "landmark_path = os.path.join(data_dir, 'list_landmarks.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attr_cloth\n",
    "- attribute_type\n",
    "    - 1 : texture_related attributes\n",
    "    - 2 : fabric_related attributes\n",
    "    - 3 : shape_related attributes\n",
    "    - 4 : part_related attributes\n",
    "    - 5 : style_related attributes\n",
    "- attribute_name으로 사전식 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of attrs : 1000\n"
     ]
    }
   ],
   "source": [
    "attr_cloth = []\n",
    "with open(attr_cloth_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt == 0:\n",
    "                print(f\"number of attrs : {line}\")\n",
    "            cnt += 1\n",
    "            continue\n",
    "        splited_line = [x for x in [l.replace(\" \", \"\") for l in line.split(\"  \")] if x != \"\"]\n",
    "        attribute_name, attribute_type = splited_line\n",
    "\n",
    "        attr_cloth.append({'attr_name': attribute_name,\n",
    "                           'attr_type': int(attribute_type)})\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/attr_cloth.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'attr_cloth.pickle'), 'wb') as f:\n",
    "    pickle.dump(attr_cloth, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'attr_cloth.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attr_name': 'a-line', 'attr_type': 3},\n",
       " {'attr_name': 'abstract', 'attr_type': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_cloth[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images : 289222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attr_img = []\n",
    "with open(attr_img_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt==0:\n",
    "                print(f\"number of images : {line}\")\n",
    "            cnt += 1\n",
    "            continue\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        splited_line = [x for x in line.split(\".jpg\") if x != '']\n",
    "        img_path, attr_labels = splited_line\n",
    "        img_path += '.jpg'\n",
    "        attr_labels = list(map(int, [x for x in attr_labels.split(' ') if x != '']))\n",
    "        attr_img.append({\n",
    "            'img_path' : img_path,\n",
    "            'attr_labels' : attr_labels\n",
    "        })\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/attr_img.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'attr_img.pickle'), 'wb') as f:\n",
    "    pickle.dump(attr_img, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'attr_img.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## category_cloth\n",
    "- category type\n",
    "    - 1 : upper_body\n",
    "    - 2 : lower_body\n",
    "    - 3 : full_body\n",
    "- category 이름, 사전순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of categories : 50\n"
     ]
    }
   ],
   "source": [
    "category_cloth = []\n",
    "with open(category_cloth_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt == 0:\n",
    "                print(f\"number of categories : {line}\")\n",
    "            cnt += 1\n",
    "            continue\n",
    "        splited_line = [x for x in [l.replace(\" \", \"\") for l in line.split(\"  \")] if x != \"\"]\n",
    "        category_name, category_type = splited_line\n",
    "\n",
    "        category_cloth.append({'category_name': category_name,\n",
    "                           'category_type': int(category_type)})\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/category_cloth.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'category_cloth.pickle'), 'wb') as f:\n",
    "    pickle.dump(category_cloth, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'category_cloth.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category_name': 'Anorak', 'category_type': 1},\n",
       " {'category_name': 'Blazer', 'category_type': 1}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_cloth[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images : 289222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_img = []\n",
    "with open(category_img_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt==0:\n",
    "                print(f\"number of images : {line}\")\n",
    "            cnt += 1\n",
    "            continue\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        splited_line = [x for x in line.split(\".jpg\") if x != '']\n",
    "        img_path, category_label = splited_line\n",
    "        img_path += '.jpg'\n",
    "        category_label = list(map(int, [x for x in category_label.split(' ') if x != '']))\n",
    "        category_img.append({\n",
    "            'img_path' : img_path,\n",
    "            'category_label' : category_label\n",
    "        })\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_img[0]['img_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_img[0]['category_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/category_img.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'category_img.pickle'), 'wb') as f:\n",
    "    pickle.dump(category_img, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'category_img.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bbox\n",
    "- format : [x1, y1, x2, y2]\n",
    "- x1, y1 : upper left point\n",
    "- x2, y2 : lower right point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images : 289222\n",
      "image_name  x_1  y_1  x_2  y_2\n"
     ]
    }
   ],
   "source": [
    "bboxes = []\n",
    "with open(bbox_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt == 0:\n",
    "                print(f\"number of images : {line}\")\n",
    "            else :\n",
    "                print(line)\n",
    "            cnt += 1\n",
    "            continue\n",
    "        img_path, bbox = line.split('.jpg')\n",
    "        img_path += '.jpg'\n",
    "        bbox = list(map(int, [x for x in bbox.split(' ') if x != '']))\n",
    "\n",
    "        bboxes.append({'img_path': img_path,\n",
    "                      'bbox': bbox})\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg',\n",
       " 'bbox': [72, 79, 232, 273]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/bboxes.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'bboxes.pickle'), 'wb') as f:\n",
    "    pickle.dump(bboxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'bboxes.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark\n",
    "- column info\n",
    "    - image name\n",
    "    - clothes type\n",
    "        - 1 : upper_body\n",
    "            - 6개의 landmark\n",
    "        - 2 : lower_body\n",
    "            - 4개의 landmark\n",
    "        - 3 : fully_body\n",
    "            - 8개의 landmark\n",
    "    - variation type\n",
    "        - 1 : normal pose\n",
    "        - 2 : medium pose\n",
    "        - 3 : large pose\n",
    "        - 4 : medium zoom-in\n",
    "        - 5 : large zoom-in\n",
    "    - list of landmark info (1 to 8)\n",
    "        - landmark visibility\n",
    "            - 0 : visible\n",
    "            - 1 : invisible/occluded\n",
    "            - 2 : truncated/cut-off\n",
    "        - landmark location x\n",
    "        - landmark location y\n",
    "        \n",
    "---\n",
    "\n",
    "- upper-body clothes : [left_collar, right_collar, left_sleeve, right_sleeve, left_hem, right_hem]\n",
    "- lower-body clothes : [left_waistline, right_waistline, left_hem, right_hem]\n",
    "- fully-body clothes : [left_collar, right_collar, left_sleeve, right_sleeve, left_waistline, right_waistline, left_hem, right_hem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_rule_dict = {\n",
    "    1 : ['left_collar', 'right_collar', 'left_sleeve', 'right_sleeve', 'left_hem', 'right_hem'],\n",
    "    2 : ['left_waistline', 'right_waistline', 'left_hem', 'right_hem '],\n",
    "    3 : ['left_collar', 'right_collar', 'left_sleeve', 'right_sleeve', 'left_waistline', 'right_waistline', 'left_hem', 'right_hem']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images : 289222\n"
     ]
    }
   ],
   "source": [
    "landmarks = []\n",
    "with open(landmark_path, 'r') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if not line : break\n",
    "        if cnt < 2 :\n",
    "            if cnt == 0:\n",
    "                print(f\"number of images : {line}\")\n",
    "            # else :\n",
    "            #     print(line)\n",
    "            cnt += 1\n",
    "            continue\n",
    "\n",
    "        img_path, landmark = line.split('.jpg')\n",
    "        img_path += '.jpg'\n",
    "        # sys.exit(1)\n",
    "        \n",
    "        landmark = list(map(int, [x for x in landmark.split(' ') if x != '']))\n",
    "        clothes_type, landmark = landmark[0], landmark[1:]\n",
    "        landmark_dict = defaultdict(list)\n",
    "        for idx, value in enumerate(landmark):\n",
    "            landmark_dict[landmark_rule_dict[clothes_type][idx//3]].append(value)\n",
    "\n",
    "        landmarks.append({\n",
    "            'img_path' : img_path,\n",
    "            'clothes_type' : clothes_type,\n",
    "            'landmark' : landmark_dict\n",
    "        })\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                         1  0 146 102  0 173 095  0 094 242  0 205 255  0 136 229  0 177 232 '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 146, 102, 0, 173, 95, 0, 94, 242, 0, 205, 255, 0, 136, 229, 0, 177, 232]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, [x for x in landmark.split(' ') if x != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg',\n",
       " 'clothes_type': 1,\n",
       " 'landmark': defaultdict(list,\n",
       "             {'left_collar': [0, 146, 102],\n",
       "              'right_collar': [0, 173, 95],\n",
       "              'left_sleeve': [0, 94, 242],\n",
       "              'right_sleeve': [0, 205, 255],\n",
       "              'left_hem': [0, 136, 229],\n",
       "              'right_hem': [0, 177, 232]})}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/289222 [00:00<00:10, 27594.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_path': 'img/Sheer_Pleated-Front_Blouse/img_00000005.jpg', 'clothes_type': 1, 'landmark': defaultdict(<class 'list'>, {'left_collar': [1, 102, 106], 'right_collar': [0, 116, 102], 'left_sleeve': [0, 63, 194], 'right_sleeve': [1, 146, 216], 'left_hem': [0, 105, 245], 'right_hem': [0, 137, 246]})}\n",
      "\n",
      "left_collar [1, 102, 106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "for lm in tqdm(landmarks):\n",
    "    for landmark_name, landmark_value in lm['landmark'].items():\n",
    "        # print(landmark_name)\n",
    "        # print(landmark_value)\n",
    "        if landmark_value[0] != 0:\n",
    "            print(lm)\n",
    "            print()\n",
    "            print(landmark_name, landmark_value)\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark_map = [\"left collar\", \"right collar\", \"left sleeve\", \"right sleeve\", \"left waistline\", \"right waistline\", \"left hem\", \"right hem\"]\n",
    "len(landmark_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/landmarks.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(saved_dir, 'landmarks.pickle'), 'wb') as f:\n",
    "    pickle.dump(landmarks, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(os.path.join(saved_dir, 'landmarks.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 확인\n",
    "- 이미지 정보를 dictionary형태로 만든다\n",
    "    ```python\n",
    "    {\n",
    "        'img_path' : {\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attr_cloth.pickle',\n",
       " 'attr_img.pickle',\n",
       " 'bboxes.pickle',\n",
       " 'category_cloth.pickle',\n",
       " 'category_img.pickle',\n",
       " 'landmarks.pickle']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_paths = glob('/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_paths = [x for x in pickle_paths if 'cloth' in x]\n",
    "img_data_paths = [x for x in pickle_paths if 'cloth' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/attr_cloth.pickle',\n",
       " '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/category_cloth.pickle']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/attr_img.pickle',\n",
       " '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/bboxes.pickle',\n",
       " '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/category_img.pickle',\n",
       " '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/landmarks.pickle']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr\n",
      "file opening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289222/289222 [00:02<00:00, 100992.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bboxes\n",
      "file opening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289222/289222 [00:00<00:00, 1873192.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "category\n",
      "file opening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289222/289222 [00:00<00:00, 1836010.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "landmarks\n",
      "file opening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289222/289222 [00:00<00:00, 1475235.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_img_dict = defaultdict(dict)\n",
    "\n",
    "for img_data_path in img_data_paths:\n",
    "    # print(os.path.basename(img_data_path))\n",
    "    print(os.path.basename(img_data_path).replace('.pickle','').split('_')[0])\n",
    "    task_name = os.path.basename(img_data_path).replace('.pickle','').split('_')[0]\n",
    "    if task_name == 'bboxes':\n",
    "        task_name = 'bbox'\n",
    "    elif task_name == 'landmarks':\n",
    "        task_name = 'landmark'\n",
    "    \n",
    "    print('file opening...')\n",
    "    with open(img_data_path, 'rb') as f:\n",
    "        datas = pickle.load(f)\n",
    "    \n",
    "    # collect datas\n",
    "    for data in tqdm(datas):\n",
    "        img_path = data['img_path']\n",
    "        for key, value in data.items():\n",
    "            if key == 'img_path':\n",
    "                continue\n",
    "            if task_name == 'landmark':\n",
    "                total_img_dict[img_path]\n",
    "            elif task_name in key:\n",
    "                total_img_dict[img_path][task_name] = value\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img/Sheer_Pleated-Front_Blouse/img_00000001.jpg\n",
      "{\n",
      "  \"attr\": [\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1,\n",
      "    -1\n",
      "  ],\n",
      "  \"bbox\": [\n",
      "    72,\n",
      "    79,\n",
      "    232,\n",
      "    273\n",
      "  ],\n",
      "  \"category\": [\n",
      "    3\n",
      "  ],\n",
      "  \"landmark\": {\n",
      "    \"left_collar\": [\n",
      "      0,\n",
      "      146,\n",
      "      102\n",
      "    ],\n",
      "    \"right_collar\": [\n",
      "      0,\n",
      "      173,\n",
      "      95\n",
      "    ],\n",
      "    \"left_sleeve\": [\n",
      "      0,\n",
      "      94,\n",
      "      242\n",
      "    ],\n",
      "    \"right_sleeve\": [\n",
      "      0,\n",
      "      205,\n",
      "      255\n",
      "    ],\n",
      "    \"left_hem\": [\n",
      "      0,\n",
      "      136,\n",
      "      229\n",
      "    ],\n",
      "    \"right_hem\": [\n",
      "      0,\n",
      "      177,\n",
      "      232\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for img_path, img_info in total_img_dict.items():\n",
    "    print(img_path)\n",
    "    print(json.dumps(img_info, indent=2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/preprocessed_data.pickle\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(saved_dir, 'preprocessed_data.pickle')\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(total_img_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
