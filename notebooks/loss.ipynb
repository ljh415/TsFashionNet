{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57635569-cc8c-4684-a2b7-144b946ef6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1785b04d-659f-4813-bc2f-dae06dce2e72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LANDMARK_MAP = ['left_collar', 'right_collar', 'left_sleeve', 'right_sleeve',\n",
    "                'left_waistline', 'right_waistline', 'left_hem', 'right_hem']\n",
    "IMG_DIR = '/media/jaeho/SSD/datasets/deepfashion/img-001/'\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        with open(data_path, 'rb') as f:\n",
    "            raw_dict = pickle.load(f)\n",
    "        self.raw_data = list(raw_dict.items())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def _make_visibility(self, landmark_info):\n",
    "        visibility = []\n",
    "        for lm in LANDMARK_MAP:\n",
    "            if lm in landmark_info:\n",
    "                visibility.append(1 if landmark_info[lm][0] == 0 else 0)\n",
    "            else:\n",
    "                visibility.append(0)\n",
    "        return visibility\n",
    "    \n",
    "    def _make_landmark(self, img_size, landmark_info):\n",
    "        height, width = img_size\n",
    "        heatmap_size = img_size\n",
    "        visibility = self._make_visibility(landmark_info)\n",
    "        \n",
    "        nof_joints = len(LANDMARK_MAP)\n",
    "        joints_vis = np.array([[x] for x in visibility])\n",
    "        \n",
    "        target = np.zeros((nof_joints, width, height), dtype=np.float32)\n",
    "        target_weight = np.ones((nof_joints, 1), dtype=np.float32)\n",
    "        \n",
    "        joints = [landmark_info[x][1:] if x in landmark_info else [0, 0] for x in LANDMARK_MAP]\n",
    "        joints = np.array(joints, dtype=np.float32)\n",
    "        \n",
    "        heatmap_sigma = 3\n",
    "        tmp_size = heatmap_sigma * 3\n",
    "        feat_stride = np.asarray(img_size) / np.asarray(heatmap_size)\n",
    "        \n",
    "        for joint_id in range(nof_joints):\n",
    "            mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\n",
    "            mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\n",
    "            \n",
    "            ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
    "            br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
    "            \n",
    "            if ul[0] >= heatmap_size[0] or ul[1] >= heatmap_size[1] or br[0] < 0 or br[1] < 0:\n",
    "                target_weight[joint_id] = 0\n",
    "                continue\n",
    "            \n",
    "            # generate gaussian\n",
    "            size = 2 * tmp_size + 1\n",
    "            x = np.arange(0, size, 1, np.float32)\n",
    "            y = x[:, np.newaxis]\n",
    "            x0 = y0 = size // 2\n",
    "            \n",
    "            # the gaussian is not normalized, we want the center value to equal 1\n",
    "            g = np.exp(- ((x-x0) ** 2 + (y - y0) ** 2) / (2 * heatmap_sigma ** 2))\n",
    "            \n",
    "            # usable gaussian range\n",
    "            g_x = max(0, -ul[0]), min(br[0], heatmap_size[0]) - ul[0]\n",
    "            g_y = max(0, -ul[1]), min(br[1], heatmap_size[1]) - ul[1]\n",
    "            \n",
    "            # image rnage\n",
    "            img_x = max(0, ul[0]), min(br[0], heatmap_size[0])\n",
    "            img_y = max(0, ul[1]), min(br[1], heatmap_size[1])\n",
    "\n",
    "            v = target_weight[joint_id]\n",
    "\n",
    "            if v > 0.5:\n",
    "                target[joint_id][img_y[0] : img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
    "        return target, visibility\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path, data_dict = self.raw_data[index]\n",
    "        landmark_info = data_dict['landmark']\n",
    "        \n",
    "        img_path = os.path.join(IMG_DIR, img_path)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        landmark, visibility = self._make_landmark(img.size, landmark_info)\n",
    "        \n",
    "        return img, landmark, visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aea62a-460b-41f0-9c9c-e89fa0d0ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    if 'json' in path:\n",
    "        with open(path, 'r') as f:\n",
    "            out = json.load(f)\n",
    "    elif 'pickle' in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            out = pickle.load(f)\n",
    "    elif 'jpg' in path:\n",
    "        out = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c6681f-1d95-46ce-95b6-7692602d6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/media/jaeho/SSD/datasets/deepfashion/preprocessed_data/preprocessed_data.pickle'\n",
    "preprocessed_data = open_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8274c984-9f8b-4dd3-987b-01fb8a46eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShapeDataset('/media/jaeho/SSD/datasets/deepfashion/split/valid.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72ebda7-51b4-44a6-b0fe-3eb03dfaf98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, landmark, visibility = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8ccb30-0f93-4f06-af6c-da0eced68586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 250, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d133b6-c01e-4444-93d8-05725ddf8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.ones(dtype=np.float32, shape=landmark.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa908c3-bd9c-420e-b9b5-c3f86b067898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 250, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b76e53-f500-4ff5-8199-4d92ae7b0aab",
   "metadata": {},
   "source": [
    "$L_{landmark}=\\sum^K_{k=1}{v_k^{GT}}\\sum{||S_k{(x,y)}-S^{GT}_k{(x,y)}||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da08bd5-7cd9-4c8b-ab37-d91d55eb066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for idx, v in enumerate(visibility):\n",
    "    if v:\n",
    "        # print()\n",
    "        ll = out[idx] - landmark[idx]\n",
    "        loss += torch.norm(torch.Tensor(ll))\n",
    "        # break\n",
    "        # plt.imshow(ll)\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.title(f\"gt-{idx}\")\n",
    "        # plt.imshow(landmark[idx])\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.title(f\"out-{idx}\")\n",
    "        # plt.imshow(out[idx])\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fd6ca8-96e2-40ac-931b-e39e633d9489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1340.5066)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478f9f50-96fe-4096-a0bd-200c4b5bf785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea8db765-eab1-4db0-8851-87aaeba3d656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(223.4178)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.Tensor(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f680aca4-6ae6-4d07-9aa3-f0529ad1fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1363, 14.1271, 14.1080, 14.0736, 14.0209, 13.9544, 13.8870, 13.8342,\n",
       "        13.8041, 13.7949, 13.8041, 13.8342, 13.8870, 13.9544, 14.0209, 14.0736,\n",
       "        14.1080, 14.1271, 14.1363, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.Tensor(ll), dim=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f5b381-fc0a-4b2e-99b9-3b22249da309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3532.5125)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(torch.norm(torch.Tensor(ll), dim=1, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1948cce2-bf1f-4a2d-9abf-0b367f3dfa3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1363, 14.1271, 14.1080, 14.0736, 14.0209, 13.9544, 13.8870, 13.8342,\n",
       "        13.8041, 13.7949, 13.8041, 13.8342, 13.8870, 13.9544, 14.0209, 14.0736,\n",
       "        14.1080, 14.1271, 14.1363, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421,\n",
       "        14.1421, 14.1421])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(torch.Tensor(ll), dim=1, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95e885-d8b9-4de1-9b4d-7cdfd9ad17cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b15d427-7a37-489d-9067-eb4d2d9668de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(223.4178)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum([x**2 for x in torch.linalg.norm(torch.Tensor(ll), dim=1, ord=2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80fce3-bafb-465b-914e-f2ea7b7c4516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b937d0-e858-49a7-b9c4-43965f4decbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d81336-64c8-438f-a89b-d89af933a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_loss(loc_out, vis_out, gt):\n",
    "    loss = 0\n",
    "    for idx, v in enumerate(vis_out):\n",
    "        if v:\n",
    "            ll = loc_out[idx] - gt[idx]\n",
    "            loss += sum(torch.norm(torch.Tensor(ll), dim=1, p=2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55b46b9-f773-4f9c-b984-9f033fbcaa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_loss = landmark_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c28bd9-0f3e-4755-8df5-2a75ade5fe0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21195.0742)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss(out, visibility, landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd5c400-6a23-4ed0-8fb3-ce55a7f2eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba56f615-5582-4391-b8e4-f7c9e3b7dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1644970-0596-45d4-a841-22a6fcfb4efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b6f62be8f23f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "lm_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a35b19-83a2-46ff-8bda-fa473b03056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LandmarkLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, loc_out, vis_out, gt):\n",
    "        loss = 0\n",
    "        for idx, v in enumerate(vis_out):\n",
    "            if v:\n",
    "                ll = loc_out[idx] - gt[idx]\n",
    "                loss += sum(torch.norm(torch.Tensor(ll), dim=1, p=2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e9fc234-c0f6-4d51-bd72-7bd94931da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lm_loss = LandmarkLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4229307-66cd-4e0e-8fdc-99c9defadf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21195.0742)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lm_loss(out, visibility, landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "382d7c96-4acb-460f-9fd5-18fdf51cb11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LandmarkLoss()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lm_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8186f08-811e-4651-a7b7-7d9ab176e90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21195.0742)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lm_loss(out, visibility, landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bc65b-3e66-49b7-bd83-ef4cde8eafb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb1ce5-3dec-4760-8516-826339679ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8f1b4-695e-473a-83eb-fdc59a0073be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aa22a63-c182-4d40-ad95-fd52f2c20341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 250, 200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "790dd6e6-d3d5-4227-bb77-ad55b553c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd75cf32-5b65-4162-9392-6455d3819b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97edf039-d6b1-42f7-8e94-5f54e7823834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(img).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd0205f2-e842-4dc8-9e5d-7545bc39aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lm = torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f9b6bb4-0196-44a6-a9c2-0d86cc6f5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 200)\n",
      "torch.Size([1, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([2, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([3, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([4, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([5, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([6, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([7, 224, 224])\n",
      "(250, 200)\n",
      "torch.Size([8, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.Tensor()\n",
    "for idx, lm in enumerate(landmark):\n",
    "    lm = transforms.ToPILImage()(lm)\n",
    "    lm = tf(lm)\n",
    "\n",
    "    if idx == 0:\n",
    "        new_lm = lm\n",
    "    else :\n",
    "        new_lm = torch.cat([new_lm, lm], axis=0)\n",
    "\n",
    "        \n",
    "    print(new_lm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272d5d3-e004-4344-bf43-2dd6960c433e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73fd0b-83f4-4cfb-9500-9a9718be4b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b08c87-0af7-4405-ae16-6524d7165869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(250, 200)\n",
      "<class 'numpy.ndarray'>\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "for idx, lm in enumerate(landmark):\n",
    "    print(idx)\n",
    "    print(lm.shape)\n",
    "    print(type(lm))\n",
    "    lm = np.resize(lm, (10, 10))\n",
    "    print(lm.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a4098-9617-4d82-8fe2-d08ce5dd88c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
